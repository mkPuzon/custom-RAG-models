{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b537cbac",
   "metadata": {},
   "source": [
    "# Docling Pipeline with a PostgreSQL Database\n",
    "Each chunk is embedded using `pgvector` and logs the following metadata:\n",
    "```json\n",
    "{\n",
    "  \"page_numbers\": [5],\n",
    "  \"headings\": [\"Q3 Performance\", \"Revenue Breakdown\"],\n",
    "  \"origin\": \"annual_report.pdf\"\n",
    "}\n",
    "```\n",
    "Unlike pure vector databases, this allows for hybrid searches using embeddings and SQL queries. For example we can select vectors similar to \"quaterly earnings\" but only where metadata.headings = \"Q3\". Here's the setup code for the database:\n",
    "```sql\n",
    "-- enable the pgvector extension\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS document_chunks (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  doc_filename TEXT NOT NULL,\n",
    "  -- The actual content (text, table markdown, or image caption)\n",
    "  chunk_content TEXT NOT NULL,\n",
    "  -- Rich metadata (Page #, bounding box, section header context)\n",
    "  metadata JSONB DEFAULT '{}'::jsonb,\n",
    "  -- Vector embedding (assuming 384 dimensions for all-MiniLM-L6-v2)\n",
    "  embedding vector(384),\n",
    "  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- create a specialized index for fast similarity search (HNSW)\n",
    "CREATE INDEX ON document_chunks USING hnsw (embedding vector_l2_ops);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8607d",
   "metadata": {},
   "source": [
    "## sqlalchemy Setup\n",
    "sqlalchemy provides type saftey through Pydantic and connection pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sqlalchemy import create_engine, Text, JSON, text\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, Session\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DB_URL_DOC\")\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "class DocumentChunk(Base):\n",
    "    __tablename__ = \"document_chunks\"\n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    doc_filename: Mapped[str] = mapped_column(Text)\n",
    "    metadata_: Mapped[dict] = mapped_column(\"metadata\", JSON)\n",
    "    embedding: Mapped[list[float]] = mapped_column(Vector(384))\n",
    "\n",
    "def init_db():\n",
    "    '''Set up table. '''\n",
    "    engine = create_engine(DB_URL)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))\n",
    "        conn.commit()\n",
    "    Base.metadata.create_all(engine)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f94e44",
   "metadata": {},
   "source": [
    "## Docling Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05756313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for handling graphs & images, enable VLM pipeline in Docling\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, PictureDescriptionVlmOptions\n",
    "\n",
    "def get_visual_converter():\n",
    "    '''\n",
    "    Creates a Docling converter with vision capabilities enabled for graphs and images.\n",
    "    '''\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.do_ocr = True\n",
    "    pipeline_options.do_table_structure = True\n",
    "    pipeline_options.do_picture_description = True\n",
    "\n",
    "    # select and prompt model for image to text\n",
    "    pipeline_options.picture_description_options = PictureDescriptionVlmOptions(\n",
    "        repo_id=\"HuggingFaceTB/SmolVLM-256M-Instruct\", \n",
    "        prompt=\"Analyze this image. If it is a graph or chart, detail the trends, X/Y axes, and key data points. If it is a photo, describe the scene.\"\n",
    "    )\n",
    "\n",
    "    converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "    return converter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22251baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 17:11:27,001 - INFO - Use pytorch device_name: cpu\n",
      "2025-12-21 17:11:27,005 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def process_and_chunk(doc_result):\n",
    "    '''\n",
    "    Chunks a coverted Docling result and generates embeddings.\n",
    "    '''\n",
    "    doc = doc_result.document\n",
    "    filename = doc_result.input.file.name\n",
    "\n",
    "    chunker = HybridChunker(\n",
    "        tokenizer=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_tokens=512, # target chunk size\n",
    "        merge_peers=True # merges small items like bulleted lists\n",
    "    )\n",
    "    chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "    processed_chunks = []\n",
    "    for chunk in chunk_iter:\n",
    "        text_content = chunk.text\n",
    "        # metadata stored as json in psql\n",
    "        meta = {\n",
    "            \"page_numbers\": sorted(list(set(\n",
    "                prov.page_no \n",
    "                for item in chunk.meta.doc_items \n",
    "                for prov in item.prov\n",
    "            ))),\n",
    "            \"headings\": chunk.metadata.headings,\n",
    "            \"source\": filename\n",
    "        }\n",
    "        # generate vector embedding\n",
    "        vector = embed_model.encode(text_content).tolist()\n",
    "        \n",
    "        processed_chunks.append({\n",
    "            \"filename\": filename,\n",
    "            \"content\": text_content,\n",
    "            \"metadata\": meta,\n",
    "            \"vector\": vector\n",
    "        })\n",
    "\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5095f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_ingestion(pdf_paths):\n",
    "\n",
    "    engine = init_db()\n",
    "    converter = get_visual_converter()\n",
    "\n",
    "    conversion_results = converter.convert_all(pdf_paths)\n",
    "\n",
    "    with Session(engine) as session:\n",
    "        for result in conversion_results:\n",
    "            try:\n",
    "                filename = result.input.file.name\n",
    "                print(f\"Attempting to convert {filename}...\")\n",
    "                # chunk file content\n",
    "                chunks_data = process_and_chunk(result)\n",
    "\n",
    "                orm_objs = []\n",
    "                for chunk in chunks_data:\n",
    "                    doc_chunk = DocumentChunk(\n",
    "                        doc_filename=chunk[\"filename\"],\n",
    "                        chunk_content=chunk[\"content\"],\n",
    "                        metadata_=chunk[\"metadata\"], \n",
    "                        embedding=chunk[\"vector\"]\n",
    "                    )\n",
    "                    orm_objs.append(doc_chunk)\n",
    "\n",
    "                session.add_all(orm_objs)\n",
    "                session.flush()\n",
    "                print(f\"Staged {len(orm_objs)} chunks.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to process {filename}: {e}\")\n",
    "                session.rollback() # undo changes for this specific file\n",
    "        session.commit()\n",
    "        print(\"[S] Data ingestion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601aab3f",
   "metadata": {},
   "source": [
    "## Running the Full Docling + PostgreSQL RAG Pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33a4568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files: ['../custom-RAG-models/corpuses/pdfs/Layton2022EstimatingHeading.pdf', '../custom-RAG-models/corpuses/pdfs/Layton2022DistributedEncoding.pdf', '../custom-RAG-models/corpuses/pdfs/Warren2021OpticFlowWalking.pdf', '../custom-RAG-models/corpuses/pdfs/Layton2016NeuralModelofMST.pdf', '../custom-RAG-models/corpuses/pdfs/Perrone2018VisualVestibularEstimation.pdf', '../custom-RAG-models/corpuses/pdfs/Layton2012MotionPoolingModel.pdf']\n",
      "First file: ../custom-RAG-models/corpuses/pdfs/Layton2022EstimatingHeading.pdf\n"
     ]
    }
   ],
   "source": [
    "# running full ingestion pipline\n",
    "from pathlib import Path\n",
    "data_fp = Path(\"../custom-RAG-models/corpuses/pdfs\")\n",
    "files_to_process = [\n",
    "    str(p) for p in data_fp.glob(\"*.pdf\")\n",
    "]\n",
    "print(f\"Found {len(files_to_process)} files: {files_to_process}\")\n",
    "print(f\"First file: {files_to_process[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7659846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 17:11:29,425 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 17:11:29,681 - INFO - Going to convert document batch...\n",
      "2025-12-21 17:11:29,685 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 5dd2eb934328b25705639acdb714a5a7\n",
      "2025-12-21 17:11:29,723 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-21 17:11:29,738 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-21 17:11:29,848 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 17:11:33,837 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-21 17:11:33,850 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-21 17:11:33,853 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-21 17:11:33,857 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-21 17:11:34,643 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:34,707 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:34,730 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:34,848 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:34,851 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,366 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,368 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,378 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,380 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,575 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,576 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,768 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 17:11:35,770 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-21 17:11:36,686 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-21 17:11:36,702 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-21 17:11:36,721 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-21 17:11:36,783 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 17:11:37,440 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-21 17:11:37,444 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-21 17:11:37,570 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 17:11:39,596 - INFO - Processing document Layton2022EstimatingHeading.pdf\n",
      "2025-12-21 18:32:29,021 - INFO - Finished converting document Layton2022EstimatingHeading.pdf in 4859.61 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to convert Layton2022EstimatingHeading.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] Failed to process Layton2022EstimatingHeading.pdf: 'DocChunk' object has no attribute 'metadata'\n",
      "[S] Data ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "rag_ingestion([files_to_process[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
